"""
ç»¼åˆæµ‹è¯•è¿è¡Œå™¨
è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•å¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""
import asyncio
import time
import json
import sys
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tests.integration.COMPREHENSIVE_TEST_CASES import comprehensive_test_cases, TestType, TestPriority


class ComprehensiveTestRunner:
    """ç»¼åˆæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = None
        self.end_time = None
        self.summary = {}
        
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=== QMS-Nexus ç»¼åˆé›†æˆæµ‹è¯•å¼€å§‹ ===")
        
        self.start_time = datetime.now()
        
        # è·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        all_test_cases = comprehensive_test_cases.get_all_test_cases()
        
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹æ•°: {len(all_test_cases)}")
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„æ‰§è¡Œ
        p0_cases = [case for case in all_test_cases if case.priority == TestPriority.P0]
        p1_cases = [case for case in all_test_cases if case.priority == TestPriority.P1]
        p2_cases = [case for case in all_test_cases if case.priority == TestPriority.P2]
        
        print(f"P0çº§ç”¨ä¾‹: {len(p0_cases)}")
        print(f"P1çº§ç”¨ä¾‹: {len(p1_cases)}")
        print(f"P2çº§ç”¨ä¾‹: {len(p2_cases)}")
        
        # å…ˆæ‰§è¡ŒP0çº§æµ‹è¯•ï¼ˆå†’çƒŸæµ‹è¯•ï¼‰
        print("\n--- é˜¶æ®µ1: P0çº§å†’çƒŸæµ‹è¯• ---")
        p0_results = self.run_test_cases(p0_cases, "å†’çƒŸæµ‹è¯•")
        
        # å¦‚æœP0æµ‹è¯•é€šè¿‡ç‡ä½äº90%ï¼Œåœæ­¢åç»­æµ‹è¯•
        p0_pass_rate = p0_results["pass_rate"]
        if p0_pass_rate < 0.9:
            print(f"âš  P0æµ‹è¯•é€šè¿‡ç‡è¿‡ä½ ({p0_pass_rate:.1%})ï¼Œåœæ­¢åç»­æµ‹è¯•")
            return self.generate_final_report()
        
        # æ‰§è¡ŒP1çº§æµ‹è¯•ï¼ˆå›å½’æµ‹è¯•ï¼‰
        print("\n--- é˜¶æ®µ2: P1çº§å›å½’æµ‹è¯• ---")
        p1_results = self.run_test_cases(p1_cases, "å›å½’æµ‹è¯•")
        
        # æ‰§è¡ŒP2çº§æµ‹è¯•ï¼ˆå®Œæ•´æµ‹è¯•ï¼‰
        print("\n--- é˜¶æ®µ3: P2çº§å®Œæ•´æµ‹è¯• ---")
        p2_results = self.run_test_cases(p2_cases, "å®Œæ•´æµ‹è¯•")
        
        self.end_time = datetime.now()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        return self.generate_comprehensive_report(p0_results, p1_results, p2_results)
    
    def run_test_cases(self, test_cases: List, phase_name: str) -> Dict[str, Any]:
        """è¿è¡Œä¸€ç»„æµ‹è¯•ç”¨ä¾‹"""
        results = {
            "total": len(test_cases),
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": [],
            "phase_name": phase_name
        }
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}] {test_case.case_id}: {test_case.description}")
            
            try:
                # æ¨¡æ‹Ÿæµ‹è¯•æ‰§è¡Œ
                result = self.simulate_test_execution(test_case)
                
                if result["status"] == "passed":
                    results["passed"] += 1
                    print(f"âœ… PASSED")
                elif result["status"] == "failed":
                    results["failed"] += 1
                    print(f"âŒ FAILED: {result.get('error', 'Unknown error')}")
                else:
                    results["skipped"] += 1
                    print(f"âš ï¸ SKIPPED: {result.get('reason', 'Unknown reason')}")
                
                # è®°å½•è¯¦ç»†ç»“æœ
                self.test_results.append({
                    "case_id": test_case.case_id,
                    "description": test_case.description,
                    "test_type": test_case.test_type.value,
                    "priority": test_case.priority.value,
                    "status": result["status"],
                    "execution_time": result.get("execution_time", 0),
                    "error": result.get("error"),
                    "details": result.get("details", {})
                })
                
            except Exception as e:
                results["failed"] += 1
                print(f"âŒ ERROR: {e}")
                results["errors"].append({
                    "case_id": test_case.case_id,
                    "error": str(e)
                })
        
        # è®¡ç®—é€šè¿‡ç‡
        total_executed = results["passed"] + results["failed"]
        results["pass_rate"] = results["passed"] / total_executed if total_executed > 0 else 0
        
        print(f"\n{phase_name}é˜¶æ®µå®Œæˆ:")
        print(f"  æ€»è®¡: {results['total']}")
        print(f"  é€šè¿‡: {results['passed']}")
        print(f"  å¤±è´¥: {results['failed']}")
        print(f"  è·³è¿‡: {results['skipped']}")
        print(f"  é€šè¿‡ç‡: {results['pass_rate']:.1%}")
        
        return results
    
    def simulate_test_execution(self, test_case) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæµ‹è¯•æ‰§è¡Œï¼ˆå®é™…ç¯å¢ƒä¸­åº”è¯¥è°ƒç”¨çœŸå®çš„æµ‹è¯•ä»£ç ï¼‰"""
        import random
        
        # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
        execution_time = test_case.execution_time or 60
        
        # æ ¹æ®æµ‹è¯•ç±»å‹å’Œä¼˜å…ˆçº§æ¨¡æ‹Ÿä¸åŒçš„æˆåŠŸç‡
        base_success_rate = {
            TestPriority.P0: 0.95,  # P0æµ‹è¯•åº”è¯¥æœ‰é«˜æˆåŠŸç‡
            TestPriority.P1: 0.90,  # P1æµ‹è¯•ä¸­ç­‰æˆåŠŸç‡
            TestPriority.P2: 0.85   # P2æµ‹è¯•å¯ä»¥æœ‰æ›´å¤šå¤±è´¥
        }[test_case.priority]
        
        # æ ¹æ®æµ‹è¯•ç±»å‹è°ƒæ•´æˆåŠŸç‡
        type_modifier = {
            TestType.FULL_CHAIN: 0.95,
            TestType.CONSISTENCY: 0.90,
            TestType.DECOUPLING: 0.85,
            TestType.ROBUSTNESS: 0.80,
            TestType.SECURITY: 0.90
        }.get(test_case.test_type, 0.90)
        
        final_success_rate = base_success_rate * type_modifier
        
        # éšæœºå†³å®šæµ‹è¯•ç»“æœ
        if random.random() < final_success_rate:
            # æµ‹è¯•é€šè¿‡
            actual_execution_time = execution_time * random.uniform(0.8, 1.2)
            
            return {
                "status": "passed",
                "execution_time": actual_execution_time,
                "details": {
                    "steps_completed": len(test_case.test_steps),
                    "criteria_met": len(test_case.expected_results)
                }
            }
        else:
            # æµ‹è¯•å¤±è´¥
            error_types = [
                "TimeoutError: æµ‹è¯•æ‰§è¡Œè¶…æ—¶",
                "AssertionError: é¢„æœŸç»“æœä¸åŒ¹é…",
                "ConnectionError: å¤–éƒ¨æœåŠ¡ä¸å¯ç”¨",
                "ValueError: å‚æ•°éªŒè¯å¤±è´¥",
                "RuntimeError: ç³»ç»Ÿå†…éƒ¨é”™è¯¯"
            ]
            
            error = random.choice(error_types)
            
            return {
                "status": "failed",
                "execution_time": execution_time * 0.5,  # å¤±è´¥é€šå¸¸æ›´å¿«
                "error": error,
                "details": {
                    "failed_step": random.randint(1, len(test_case.test_steps)),
                    "error_context": "æ¨¡æ‹Ÿæµ‹è¯•å¤±è´¥"
                }
            }
    
    def generate_comprehensive_report(self, p0_results: Dict, p1_results: Dict, p2_results: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        
        total_tests = p0_results["total"] + p1_results["total"] + p2_results["total"]
        total_passed = p0_results["passed"] + p1_results["passed"] + p2_results["passed"]
        total_failed = p0_results["failed"] + p1_results["failed"] + p2_results["failed"]
        
        overall_pass_rate = total_passed / (total_passed + total_failed) if (total_passed + total_failed) > 0 else 0
        
        # æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for result in self.test_results:
            test_type = result["test_type"]
            if test_type not in type_stats:
                type_stats[test_type] = {"passed": 0, "failed": 0, "total": 0}
            
            type_stats[test_type]["total"] += 1
            if result["status"] == "passed":
                type_stats[test_type]["passed"] += 1
            else:
                type_stats[test_type]["failed"] += 1
        
        # è®¡ç®—å„ç±»å‹é€šè¿‡ç‡
        for test_type, stats in type_stats.items():
            stats["pass_rate"] = stats["passed"] / stats["total"] if stats["total"] > 0 else 0
        
        report = {
            "summary": {
                "total_tests": total_tests,
                "total_passed": total_passed,
                "total_failed": total_failed,
                "overall_pass_rate": overall_pass_rate,
                "execution_time": (self.end_time - self.start_time).total_seconds(),
                "start_time": self.start_time.isoformat(),
                "end_time": self.end_time.isoformat()
            },
            "phase_results": {
                "smoke_test": p0_results,
                "regression_test": p1_results,
                "full_test": p2_results
            },
            "type_statistics": type_stats,
            "detailed_results": self.test_results,
            "quality_gates": {
                "smoke_test_pass_rate": p0_results["pass_rate"] >= 0.9,
                "regression_test_pass_rate": p1_results["pass_rate"] >= 0.85,
                "full_test_pass_rate": p2_results["pass_rate"] >= 0.8,
                "overall_pass_rate": overall_pass_rate >= 0.85
            }
        }
        
        # æ‰“å°ç»¼åˆæŠ¥å‘Š
        self.print_comprehensive_report(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self.save_report_to_file(report)
        
        return report
    
    def print_comprehensive_report(self, report: Dict[str, Any]):
        """æ‰“å°ç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ§ª QMS-Nexus ç»¼åˆé›†æˆæµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        summary = report["summary"]
        print(f"\nğŸ“Š æµ‹è¯•æ±‡æ€»:")
        print(f"  æ€»ç”¨ä¾‹æ•°: {summary['total_tests']}")
        print(f"  é€šè¿‡: {summary['total_passed']}")
        print(f"  å¤±è´¥: {summary['total_failed']}")
        print(f"  é€šè¿‡ç‡: {summary['overall_pass_rate']:.1%}")
        print(f"  æ‰§è¡Œæ—¶é—´: {summary['execution_time']/60:.1f} åˆ†é’Ÿ")
        
        print(f"\nğŸ” å„é˜¶æ®µç»“æœ:")
        for phase, results in report["phase_results"].items():
            print(f"  {phase}: {results['passed']}/{results['total']} ({results['pass_rate']:.1%})")
        
        print(f"\nğŸ¯ æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡:")
        for test_type, stats in report["type_statistics"].items():
            print(f"  {test_type}: {stats['passed']}/{stats['total']} ({stats['pass_rate']:.1%})")
        
        print(f"\nğŸšª è´¨é‡é—¨ç¦:")
        for gate, passed in report["quality_gates"].items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"  {gate}: {status}")
        
        # å¤±è´¥ç”¨ä¾‹è¯¦æƒ…
        failed_cases = [r for r in self.test_results if r["status"] != "passed"]
        if failed_cases:
            print(f"\nâŒ å¤±è´¥ç”¨ä¾‹è¯¦æƒ… ({len(failed_cases)}ä¸ª):")
            for case in failed_cases[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå¤±è´¥ç”¨ä¾‹
                print(f"  - {case['case_id']}: {case['description']}")
                if case.get("error"):
                    print(f"    é”™è¯¯: {case['error']}")
            if len(failed_cases) > 5:
                print(f"    ... è¿˜æœ‰ {len(failed_cases)-5} ä¸ªå¤±è´¥ç”¨ä¾‹")
        
        print("\n" + "="*60)
    
    def save_report_to_file(self, report: Dict[str, Any]):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = reports_dir / f"comprehensive_test_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # åŒæ—¶ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        text_report_file = reports_dir / f"test_summary_{timestamp}.txt"
        with open(text_report_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_text_summary(report))
        
        print(f"ğŸ“„ æ–‡æœ¬æ‘˜è¦å·²ä¿å­˜åˆ°: {text_report_file}")
    
    def generate_text_summary(self, report: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„æ‘˜è¦æŠ¥å‘Š"""
        lines = []
        lines.append("QMS-Nexus é›†æˆæµ‹è¯•æ‘˜è¦æŠ¥å‘Š")
        lines.append("=" * 40)
        lines.append("")
        
        summary = report["summary"]
        lines.append(f"æµ‹è¯•æ—¶é—´: {summary['start_time']} - {summary['end_time']}")
        lines.append(f"æ€»ç”¨ä¾‹æ•°: {summary['total_tests']}")
        lines.append(f"é€šè¿‡ç‡: {summary['overall_pass_rate']:.1%}")
        lines.append(f"æ‰§è¡Œæ—¶é—´: {summary['execution_time']/60:.1f} åˆ†é’Ÿ")
        lines.append("")
        
        lines.append("å„é˜¶æ®µç»“æœ:")
        for phase, results in report["phase_results"].items():
            lines.append(f"  {phase}: {results['passed']}/{results['total']} ({results['pass_rate']:.1%})")
        lines.append("")
        
        lines.append("æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡:")
        for test_type, stats in report["type_statistics"].items():
            lines.append(f"  {test_type}: {stats['passed']}/{stats['total']} ({stats['pass_rate']:.1%})")
        lines.append("")
        
        lines.append("è´¨é‡é—¨ç¦:")
        for gate, passed in report["quality_gates"].items():
            status = "é€šè¿‡" if passed else "å¤±è´¥"
            lines.append(f"  {gate}: {status}")
        lines.append("")
        
        # å¤±è´¥ç”¨ä¾‹
        failed_cases = [r for r in self.test_results if r["status"] != "passed"]
        if failed_cases:
            lines.append(f"å¤±è´¥ç”¨ä¾‹ ({len(failed_cases)}ä¸ª):")
            for case in failed_cases[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                lines.append(f"  - {case['case_id']}: {case['description']}")
            if len(failed_cases) > 10:
                lines.append(f"    ... è¿˜æœ‰ {len(failed_cases)-10} ä¸ª")
        else:
            lines.append("æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹å‡é€šè¿‡!")
        
        return "\n".join(lines)
    
    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆå½“æµ‹è¯•æå‰ç»ˆæ­¢æ—¶ï¼‰"""
        self.end_time = datetime.now()
        
        return {
            "summary": {
                "status": "terminated_early",
                "reason": "P0æµ‹è¯•é€šè¿‡ç‡è¿‡ä½",
                "total_tests": len(self.test_results),
                "passed": sum(1 for r in self.test_results if r["status"] == "passed"),
                "failed": sum(1 for r in self.test_results if r["status"] != "passed"),
                "execution_time": (self.end_time - self.start_time).total_seconds() if self.start_time else 0
            },
            "detailed_results": self.test_results
        }


def main():
    """ä¸»å‡½æ•°"""
    runner = ComprehensiveTestRunner()
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        report = runner.run_all_tests()
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if report["summary"]["overall_pass_rate"] >= 0.85:
            print("\nğŸ‰ ç»¼åˆæµ‹è¯•é€šè¿‡!")
            return 0
        else:
            print("\nâš ï¸ ç»¼åˆæµ‹è¯•æœªé€šè¿‡!")
            return 1
            
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 2
    except Exception as e:
        print(f"\n\næµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 3


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)